//
// Created by pyh on 24-7-10.
//
#include "permutohedral_lattice_kernel.cuh"
#include "cuda_runtime.h"
#include <stdio.h>
__device__ int HashTableGPU::insert(short *key, unsigned int slot) {
    int h = modHash(hash(key));//ä½¿ç”¨å“ˆå¸Œå‡½æ•°å°†keyâ€”â€”æŠ•å½±åçš„positionâ€”â€”çš„å‰pdç»´ï¼Œè®¡ç®—ä¸ºå“ˆå¸Œå€¼ï¼›éšåå°†å“ˆå¸Œå€¼é€šè¿‡å–ä½™æ•°æ˜ å°„ä¸ºï¼ˆ0 - 2*capacityï¼‰ä¹‹é—´çš„ä¸€ä¸ªæ•°å€¼
    while (1) {
        int *e = entries + h;// ç§»åŠ¨åˆ°entriesæ•°ç»„ä¸­çš„ä½ç½®hä¸Š

        // If the cell is empty (-1), lock it (-2)
        int contents = atomicCAS(e, -1, -2);//å¦‚æœeä¸­çš„æ•°å€¼ä¸º-1å³æ²¡æœ‰è¢«ä½¿ç”¨è¿‡ï¼Œåˆ™å°†å®ƒå˜ä¸º-2

        if (contents == -2){
            //ä¾‹å¦‚çº¿ç¨‹Aå·²ç»é”å®šäº†ä¸€ä¸ªæ²¡ç”¨è¿‡çš„entryï¼ˆeï¼‰ï¼Œè¯¥eçš„æ•°å€¼å·²ç»æ˜¯-2äº†ï¼Œå‘ç”Ÿå“ˆå¸Œå†²çªçš„çº¿ç¨‹Bå†æ¬¡é”å®šå®ƒçš„è¯ï¼Œç”±äºæœ¬æ¥eå°±æ˜¯-2,é‚£ä¹ˆå¾—åˆ°çš„contentsï¼ˆæ—§å€¼ï¼‰ä¹Ÿæ˜¯-2
            //ç”±äºè¯¥eå·²ç»è¢«é”å®šï¼Œçº¿ç¨‹Bä¸èƒ½æ“ä½œæ‰€ä»¥è¯¥ifä¸åšä»»ä½•äº‹æƒ…
            //ç›´æ¥åˆ°ä¸‹é¢å¾€åç§»åŠ¨ä¸€æ ¼ï¼Œè¿™ç§æƒ…å†µå¯èƒ½å¼•å‘å…¶å®æ²¡æœ‰å‘ç”Ÿå“ˆå¸Œå†²çªå³æŸ¥è¯¢keyå’Œkeysä¸­çš„å¯¹åº”keyæ˜¯matchçš„æƒ…å†µè¢«å½“æˆäº†å“ˆå¸Œå†²çªçš„æƒ…å†µå¤„ç†ï¼Œå¯¼è‡´entriesä¸­çš„éƒ¨åˆ†æ ¼å­æ”¾äº†é‡å¤çš„å†…å®¹
            // If it was locked already, move on to the next cell
        }else if (contents == -1) {
            // å¦‚æœè¯¥eåŸæ¥çš„å€¼ä¸º-1,åˆ™è¯´æ˜æ²¡æœ‰è¢«ä½¿ç”¨
            // If it was empty, we successfully locked it. Write our key.
            for (int i = 0; i < pd; i++) {
                keys[slot * pd + i] = key[i];//ä¿å­˜keyï¼Œå³å½“å‰æ ¼ç‚¹çš„å‰pdç»´æ•°æ®
            }
            // Unlock
            atomicExch(e, slot);//å°†å½“å‰æ ¼ç‚¹çš„æ€»ç´¢å¼•ä¿å­˜åˆ°eé‡Œï¼ŒğŸ”“è§£é”
            return h;//è¿”å›è¯¥keyåœ¨entriesä¸­çš„ç´¢å¼•
        } else {
            // å¦‚æœeæ˜¯è¢«è§£é”çš„ï¼Œå¹¶ä¸”æœ‰ç›¸åº”çš„keyåœ¨keysé‡Œï¼Œæ£€æŸ¥æ˜¯å¦åŒ¹é…
            // The cell is unlocked and has a key in it, check if it matches
            bool match = true;
            for (int i = 0; i < pd && match; i++) {
                match = (keys[contents*pd+i] == key[i]);// æ ¹æ®eä¸­å·²ç»ä¿å­˜çš„æ€»ç´¢å¼•ï¼Œåœ¨keysä¸­å¯»æ‰¾å¯¹åº”çš„keyï¼Œçœ‹æ˜¯å¦å’Œå½“å‰çš„keyä¸€æ ·
            }
            if (match)//å¦‚æœä¸€æ ·
                return h;//è¿”å›è¯¥keyåœ¨entriesä¸­çš„ç´¢å¼•
        }
        // increment the bucket with wraparound
        //å“ˆå¸Œå€¼ä¸€æ ·ï¼Œä½†æ˜¯å¯¹åº”çš„keyåˆä¸ä¸€æ ·ï¼Œåˆ™æŠŠæŒ‡å‘entriesçš„ä¸‹ä¸€ä¸ªä½ç½®ã€‚
        h++;
        if (h == capacity*2)
            h = 0;
    }
}

__global__ static void createLattice(const int n,
                                     const float *positions,
                                     const float *scaleFactor,
                                     MatrixEntry *matrix,
                                     HashTableGPU table) {

    const int idx = threadIdx.x + blockIdx.x * blockDim.x;
    if (idx >= n)
        return;

    float elevated[pd + 1];
    const float *position = positions + idx * pd;
    int rem0[pd + 1];
    int rank[pd + 1];

    // embed position vector into the hyperplane
    // first rotate position into the (pd+1)-dimensional hyperplane
    // sm contains the sum of 1..n of our feature vector
    float sm = 0;
    for (int i = pd; i > 0; i--) {
        float cf = position[i - 1] * scaleFactor[i - 1];
        elevated[i] = sm - i * cf;
        sm += cf;
    }
    elevated[0] = sm;


    // Find the closest 0-colored simplex through rounding
    // greedily search for the closest zero-colored lattice point
    short sum = 0;
    for (int i = 0; i <= pd; i++) {
        float v = elevated[i] * (1.0 / (pd + 1));
        float up = ceil(v) * (pd + 1);
        float down = floor(v) * (pd + 1);
        if (up - elevated[i] < elevated[i] - down) {
            rem0[i] = (short) up;
        } else {
            rem0[i] = (short) down;
        }
        sum += rem0[i];
    }
    sum /= pd + 1;


    // Find the simplex we are in and store it in rank (where rank describes what position coordinate i has in the sorted order of the features values)
    for (int i = 0; i <= pd; i++)
        rank[i] = 0;
    for (int i = 0; i < pd; i++) {
        double di = elevated[i] - rem0[i];
        for (int j = i + 1; j <= pd; j++)
            if (di < elevated[j] - rem0[j])
                rank[i]++;
            else
                rank[j]++;
    }

    // If the point doesn't lie on the plane (sum != 0) bring it back
    for (int i = 0; i <= pd; i++) {
        rank[i] += sum;
        if (rank[i] < 0) {
            rank[i] += pd + 1;
            rem0[i] += pd + 1;
        } else if (rank[i] > pd) {
            rank[i] -= pd + 1;
            rem0[i] -= pd + 1;
        }
    }


    float barycentric[pd + 2];
    for (int i = 0; i < pd + 2; ++i) {
        barycentric[i] = 0.0f;
    }
    // Compute the barycentric coordinates (p.10 in [Adams etal 2010])
    for (int i = 0; i <= pd; i++) {
        float delta = (elevated[i] - rem0[i]) * (1.0 / (pd + 1));
        barycentric[pd - rank[i]] += delta;
        barycentric[pd + 1 - rank[i]] -= delta;
    }
    // Wrap around
    barycentric[0] += 1.0 + barycentric[pd + 1];


    short key[pd];
    for (int remainder = 0; remainder <= pd; remainder++) {
        // Compute the location of the lattice point explicitly (all but
        // the last coordinate - it's redundant because they sum to zero)
        for (int i = 0; i < pd; i++) {
            key[i] = static_cast<short>(rem0[i] + remainder);
            if (rank[i] > pd - remainder)
                key[i] -= (pd + 1);
        }

        MatrixEntry r;
        unsigned int slot = static_cast<unsigned int>(idx * (pd + 1) + remainder);// ç´¢å¼•ï¼šä¿å­˜äº†åŒ…å›´ç¬¬idxä¸ªpositionçš„ç¬¬remainderä¸ªæ ¼ç‚¹
        r.index = table.insert(key, slot);//indexä¿å­˜å½“å‰keyåœ¨å“ˆå¸Œè¡¨entriesä¸­çš„ç´¢å¼•
        r.weight = barycentric[remainder];//ä¿å­˜äº†å½“å‰æ ¼ç‚¹å…³äºå½“å‰positionçš„é‡å¿ƒæ’å€¼çš„æƒé‡
        matrix[idx * (pd + 1) + remainder] = r;// ä¸€ä¸ªmatrixçš„æ•°ç»„ï¼Œä¿å­˜äº†åŒ…å›´ç¬¬idxä¸ªpositionçš„ç¬¬remainderä¸ªæ ¼ç‚¹åœ¨å“ˆå¸Œè¡¨ä¸­çš„indexå’Œé‡å¿ƒæƒé‡
        //å¤‡æ³¨ï¼šå¦‚æœæœ‰1,2,3,ä¸‰ä¸ªæ•°æ®ç‚¹ã€‚å®ƒä»¬æ¯ä¸ªéƒ½æœ‰3ä¸ªæ ¼ç‚¹ã€‚æ ¼ç‚¹è¿™äº›æ ¼ç‚¹æœ‰å…¬å…±çš„ã€‚å¯¹äºå…¬å…±çš„æ ¼ç‚¹ï¼Œinsertçš„æ—¶å€™ï¼Œentriesé‡Œå­˜çš„slotç†è®ºåªä¿ç•™æœ€æ—©insertçš„
    }
}

__global__ static void cleanHashTable(int n, HashTableGPU table, int * M) {
    // n = 2 * æ€»æ•°æ®ç‚¹æ•° * ï¼ˆpd+1ï¼‰
    // pd+1ä¸ºpostionçš„ç»´åº¦+1
    const int idx = (blockIdx.y * gridDim.x + blockIdx.x) * blockDim.x * blockDim.y + threadIdx.x;

    if (idx >= n)
        return;

    // entriesçš„é•¿åº¦æ˜¯2*capacity
    // capacityçš„é•¿åº¦æ˜¯ æ•°æ®ç‚¹æ•° * (pd + 1)
    // å› æ­¤næ‰éœ€è¦ä¸º2 * æ•°æ®ç‚¹æ•° * (pd + 1)
    // find my hash table entry
    int *e = table.entries + idx;//å¯¹äºentriesä¸­çš„æ‰€æœ‰å€¼

    // Check if I created my own key in the previous phase
    if (*e >= 0) {// å¯¹äºæ‰€æœ‰å®Œæˆæ’å…¥äº†çš„eè¿›è¡Œæ£€æŸ¥
        // Rehash my key and reset the pointer in order to merge with
        // any other pixel that created a different entry under the
        // same key. If the computation was serial this would never
        // happen, but sometimes race conditions can make the same key
        // be inserted twice. hashTableRetrieve always returns the
        // earlier, so it's no problem as long as we rehash now.
        // keysçš„æ€»é•¿åº¦å³ä¸ºcapacity * pd = æ•°æ®ç‚¹æ•° * (pd + 1) * pd = æ•°æ®ç‚¹æ•° * åŒ…å›´ä¸€ä¸ªpositionéœ€è¦çš„æ ¼ç‚¹æ•° * pd
        // *eå°±æ˜¯å½“å‰æ ¼ç‚¹åœ¨ã€æ•°æ®ç‚¹æ•° * åŒ…å›´ä¸€ä¸ªpositionéœ€è¦çš„æ ¼ç‚¹æ•°ã€‘ä¸­çš„ç´¢å¼•
        // è¯¥æ•°å€¼ä¹˜ä»¥pdåå°±åˆ°äº†è¯¥æ ¼ç‚¹çš„å‰pdç»´ã€ä¹Ÿå°±æ˜¯å½“å‰çš„keyã€‘åœ¨keysä¸­çš„ç¬¬ä¸€ä¸ªå€¼çš„åœ°å€
        // é‡æ–°è¿›è¡Œå“ˆå¸Œï¼Œè§£å†³äº†insertè¿‡ç¨‹ä¸­åˆ†æ”¯-2ä¸­å¯èƒ½å‡ºç°çš„é—®é¢˜â€”â€”ç›¸åŒçš„keyå ç”¨ä¸åŒçš„æ ¼å­ï¼Œæ ¼å­é‡Œé¢å­˜äº†ä¸åŒçš„slot
//        *e = table.retrieve(table.keys + *e * pd);
        int e_check = table.retrieve(table.keys + *e * pd);
        if (*e == e_check)
        {
            atomicAdd(M, 1);// å¯¹äºæ²¡æœ‰é—®é¢˜çš„ï¼Œå®ƒä»¬å°±æ˜¯ä¸é‡å¤çš„æ ¼ç‚¹ï¼Œåˆ©ç”¨atomicAddè®¡ç®—æ ¼ç‚¹æ€»æ•°
        }
        *e = e_check;// é‡æ–°å­˜å›ç¬¬ä¸€ä¸ªinsertçš„é‚£ä¸ªslot
    }
}

__global__ static void update_matrix(const int n, MatrixEntry *matrix, HashTableGPU table) {
    // nä¸ªæ•°æ®ç‚¹ï¼Œ valuesä»£è¡¨ä½ç»´è¾“å…¥ï¼Œmatrixä»£è¡¨æ¯ä¸ªæ•°æ®ç‚¹çš„æ¯ä¸ªremaideråœ¨å“ˆå¸Œè¡¨ä¸­çš„ç´¢å¼•å’Œè¯¥æ•°æ®ç‚¹å¸¦æ¥çš„é‡å¿ƒæ’å€¼ç³»æ•°ï¼Œ tableä»£è¡¨å“ˆå¸Œè¡¨
    const int idx = threadIdx.x + blockIdx.x * blockDim.x;// æ¯ä¸ªæ•°æ®ç‚¹å¹¶è¡Œ
    const int color = blockIdx.y;// remainderä¸ªæ•°å±‚é¢çš„å¹¶è¡Œ
    const bool outOfBounds = (idx >= n);//å½“å‰çº¿ç¨‹idxæ˜¯å¦è¶…è¿‡æ•°æ®ç‚¹ç‚¹æ•°ä¸Šé™

    if (!outOfBounds) {//å¯¹äºæ²¡æœ‰è¶Šå±Šçš„çº¿ç¨‹
        // convert the matrix entry from a pointer into the entries array to a pointer into the keys/values array
        matrix[idx * (pd + 1) + color].index = table.entries[matrix[idx * (pd + 1) +
                                                                    color].index];//ä¹‹å‰è¿›è¡Œè¿‡rehashå› æ­¤è¿›è¡Œå¯¹åº”çš„æ›´æ–°
    }
}
__global__ static void splatCache(const int n, const float *values, MatrixEntry *matrix, HashTableGPU table) {
// nä¸ªæ•°æ®ç‚¹ï¼Œ valuesä»£è¡¨ä½ç»´è¾“å…¥ï¼Œmatrixä»£è¡¨æ¯ä¸ªæ•°æ®ç‚¹çš„æ¯ä¸ªremaideråœ¨å“ˆå¸Œè¡¨ä¸­çš„ç´¢å¼•å’Œè¯¥æ•°æ®ç‚¹å¸¦æ¥çš„é‡å¿ƒæ’å€¼ç³»æ•°ï¼Œ tableä»£è¡¨å“ˆå¸Œè¡¨
    const int idx = threadIdx.x + blockIdx.x * blockDim.x;// æ¯ä¸ªæ•°æ®ç‚¹å¹¶è¡Œ
    const int threadId = threadIdx.x;//æ¯ä¸ªblockä¸­çš„çº¿ç¨‹ç´¢å¼•
    const int color = blockIdx.y;// remainderä¸ªæ•°å±‚é¢çš„å¹¶è¡Œ
    const bool outOfBounds = (idx >= n);//å½“å‰çº¿ç¨‹idxæ˜¯å¦è¶…è¿‡æ•°æ®ç‚¹ç‚¹æ•°ä¸Šé™

    __shared__ int sharedOffsets[BLOCK_SIZE];//å­˜å‚¨æ¯ä¸ªçº¿ç¨‹è®¡ç®—çš„å€¼å°†è¦æ›´æ–°çš„ values æ•°ç»„ä¸­çš„ä½ç½®åç§»
    __shared__ float sharedValues[BLOCK_SIZE * vd];//è¿™ä¸ªBLOCKä¸­æ‰€æœ‰ä½ç»´è¾“å…¥
    int myOffset = -1;//å½“å‰çº¿ç¨‹è®¡ç®—çš„å€¼å°†è¦æ›´æ–°çš„ä½ç½®åç§»ï¼Œåˆå§‹å€¼ä¸º -1 è¡¨ç¤ºæ— æ•ˆã€‚
    float *myValue = sharedValues + threadId * vd;//æŒ‡å‘å½“å‰BLOCKä¸­æŸä¸ªä½ç»´è¾“å…¥çš„æŒ‡é’ˆ

    if (!outOfBounds) {//å¯¹äºæ²¡æœ‰è¶Šå±Šçš„çº¿ç¨‹

        float * value = const_cast<float *>(values + idx * (vd - 1));//å½“å‰çº¿ç¨‹æŒ‡å‘å¯¹åº”çš„ä½ç»´è¾“å…¥

        MatrixEntry r = matrix[idx * (pd + 1) + color];//å–å‡ºå½“å‰æ•°æ®ç‚¹çš„ç¬¬colorä¸ªremainder

        // convert the matrix entry from a pointer into the entries array to a pointer into the keys/values array
//        matrix[idx * (pd + 1) + color].index = r.index = table.entries[r.index];//ä¹‹å‰è¿›è¡Œè¿‡rehashå› æ­¤è¿›è¡Œå¯¹åº”çš„æ›´æ–°
        // ä»ç°åœ¨å¼€å§‹ï¼Œr.indexä¸å†æ˜¯entriesçš„ç´¢å¼•è€Œæ˜¯entriesçš„é‡Œçš„å€¼slot

        // record the offset into the keys/values array in shared space
        myOffset = sharedOffsets[threadId] = r.index * vd;//è¯¥remaideræ‰€å¯¹åº”çš„valueåœ¨å“ˆå¸Œè¡¨valuesä¸­å¯¹åº”çš„ç´¢å¼•

        for (int j = 0; j < vd - 1; j++) {//ä»¥rgbå›¾ç‰‡ä¸ºä¾‹å­vd=4,vd-1=3ã€‚
            myValue[j] = value[j] * r.weight;//å½“å‰æ•°æ®ç‚¹splatåˆ°å®ƒçš„ç¬¬colorä¸ªremainderä¸Šçš„é¢œè‰²æ•°å€¼
        }
        myValue[vd - 1] = r.weight;//ç”¨æœ€åä¸€ä¸ªç»´åº¦ï¼ˆ4ï¼‰ä¿å­˜å¯¹äºå½“å‰æ•°æ®ç‚¹å’Œå®ƒçš„ç¬¬colorä¸ªremainderä¹‹é—´æƒé‡

    } else {
        sharedOffsets[threadId] = -1;
    }

    __syncthreads();

    // am I the first thread in this block to care about this key?
    if (outOfBounds)
        return;

    for (int i = 0; i < BLOCK_SIZE; i++) {
        if (i < threadId) {// myOffsetæ˜¯å½“å‰threadIdæ‰€å¯¹åº”çš„å€¼ï¼Œä»£è¡¨äº†å½“å‰threadè¦æ“ä½œçš„å¯¹è±¡ä½ç½®
            // åœ¨è¿™é‡Œæ£€æŸ¥åŒä¸€ä¸ªblockä¸‹å…¶ä»–threadï¼Œå®ƒä»¬æ‰€æ“ä½œçš„å¯¹è±¡æ˜¯å¦å’Œå½“å‰threadç›¸åŒ
            // å¦‚æœç¼–å·å°äºå½“å‰threadï¼Œé‚£ä¹ˆå®ƒä»¬çš„ä¼˜å…ˆçº§æ¯”å½“å‰threadæ›´é«˜
            // å¤„ç†çš„åˆæ˜¯åŒä¸€ä¸ªå¯¹è±¡çš„æƒ…å†µä¸‹ï¼Œå½“å‰çº¿ç¨‹åº”å½“é€€å‡ºã€‚
            if (myOffset == sharedOffsets[i]) {
                // somebody else with higher priority cares about this key
                return;
            }
        } else if (i > threadId) {
            // å¦‚æœç¼–å·å¤§äºå½“å‰threadï¼Œé‚£ä¹ˆå®ƒä»¬çš„ä¼˜å…ˆçº§æ¯”å½“å‰threadæ›´ä½
            if (myOffset == sharedOffsets[i]) {
                // someone else with lower priority cares about this key, accumulate it into mine
                // é‚£ä¹ˆå°±å°†å—åˆ°çš„splatç´¯åŠ 
                for (int j = 0; j < vd; j++) {
                    sharedValues[threadId * vd + j] += sharedValues[i * vd + j];
                }
            }
        }
    }
    //ç»è¿‡ä¸Šé¢forå¾ªç¯ä¹‹åï¼Œåº”è¯¥ç•™ä¸‹å’Œlatticeç‚¹ç›¸åŒæ•°é‡çš„çº¿ç¨‹ã€‚è€Œä¸”è¿™äº›çº¿ç¨‹ä¹‹å‰æœ‰ç€æ“ä½œè¿™äº›latticeçš„æœ€ä¼˜å…ˆç­‰çº§
    // only the threads with something to write to main memory are still going
    float *val = table.values + myOffset;//å®Œæˆäº†splatçš„æ ¼ç‚¹æ‰€å…·å¤‡çš„ä½ç»´è¾“å…¥å€¼å…ˆè¢«ä¿å­˜åœ¨å…±äº«å†…å­˜ä¸Šï¼Œä¹‹åé€šè¿‡å¦‚ä¸‹çš„å¾ªç¯å¤åˆ¶ç»™å“ˆå¸Œè¡¨çš„value
    for (int j = 0; j < vd; j++) {
        atomicAdd(val + j, myValue[j]);
    }
}

__global__ static void blur(int n, float *newValues, MatrixEntry *matrix, int color, HashTableGPU table) {
//è¿™é‡Œnä»£è¡¨äº†æ‰€æœ‰æ•°æ®ç‚¹çš„å„è‡ªd+1ä¸ªæ ¼ç‚¹ã€‚
    const int idx = (blockIdx.y * gridDim.x + blockIdx.x) * blockDim.x * blockDim.y + threadIdx.x;
    if (idx >= n)
        return;

    // Check if I'm valid
    // tableé‡Œå­˜çš„å°±æ˜¯slot
    if (matrix[idx].index != idx)//ç”±äºsplatä¸­çš„è¯¥è¯­å¥ï¼šmatrix[idxï¼ˆè¿™é‡Œçš„idxæ˜¯æ•°æ®ç‚¹ç´¢å¼•ï¼‰ * (pd + 1) + color].index = r.index = table.entries[r.index]
        return;


    // find my key and the keys of my neighbors
    short myKey[pd + 1];
    short np[pd + 1];
    short nm[pd + 1];


    for (int i = 0; i < pd; i++) {
        myKey[i] = table.keys[idx * pd + i];//å–å‡ºå½“å‰threadå¯¹åº”çš„key
        np[i] = myKey[i] + 1;
        nm[i] = myKey[i] - 1;
    }
    np[color] -= pd + 1;
    nm[color] += pd + 1;
    //è®¡ç®—å‡ºå®ƒåœ¨colorè¿™ä¸ªç»´åº¦ä¸Šçš„ä¸¤ä¸ªè¿‘é‚»

    // æ‰¾åˆ°è®¡ç®—å‡ºæ¥çš„ä¸¤ä¸ªè¿‘é‚»åœ¨å“ˆå¸Œè¡¨ä¸Šçš„ä½ç½®
    int offNp = table.retrieve(np);
    int offNm = table.retrieve(nm);

    float *valMe = table.values + vd * idx;//å–å‡ºå½“å‰threadå¯¹åº”çš„æ•°å€¼
    float *valOut = newValues + vd * idx;//æŒ‡å‘åœ¨newValuesä¸Šå’Œä¸Šé¢çš„å¯¹åº”ä½ç½®

    //in case neighbours don't exist (lattice edges) offNp and offNm are -1
    float zeros[vd]{0};//å¯¹äºè¾¹ç•Œä¸Šçš„latticeï¼Œä½¿ç”¨å…¨0æ•°ç»„
    float *valNp = zeros; //or valMe? for edges?
    float *valNm = zeros;
    if(offNp >= 0)
        valNp = table.values + vd * offNp;//æŒ‡å‘Npè¿™ä¸€è¿‘é‚»å¯¹åº”çš„æ•°å€¼
    if(offNm >= 0)
        valNm = table.values + vd * offNm;//æŒ‡å‘Nmè¿™ä¸€è¿‘é‚»å¯¹åº”çš„æ•°å€¼

//ç›®å‰æ„Ÿè§‰è™½ç„¶æœ‰é‡å¤æ“ä½œçš„æ ¼ç‚¹ä½†æ˜¯æ˜¯å¯ä»¥è¿›è¡Œè¦†ç›–çš„ï¼Œå› ä¸ºnewvalueå­˜åœ¨å¦å¤–çš„æ•°ç»„é‡Œé¢ã€‚
    for (int i = 0; i < vd; i++)
        valOut[i] = 0.25 * valNp[i] + 0.5 * valMe[i] + 0.25 * valNm[i];//å®Œæˆblur
    //valOut[i] = 0.5f * valNp[i] + 1.0f * valMe[i] + 0.5f * valNm[i];
}

__global__ static void slice(const int n, float *values, MatrixEntry *matrix, HashTableGPU table) {
//å¯¹äºæ¯ä¸€ä¸ªæ•°æ®ç‚¹
    const int idx = threadIdx.x + blockIdx.x * blockDim.x;
    if (idx >= n)
        return;

    float value[vd-1]{0};
    float weight = 0;

    for (int i = 0; i <= pd; i++) {
        MatrixEntry r = matrix[idx * (pd + 1) + i];
        float *val = table.values + r.index * vd;
        for (int j = 0; j < vd - 1; j++) {
            value[j] += r.weight * val[j];
        }
        weight += r.weight * val[vd - 1];
    }

    for (int j = 0; j < vd - 1; j++)
        values[idx * (vd - 1) + j] = value[j];
//    weight = 1.0 / weight;
//    weight = 1.0;
//    for (int j = 0; j < vd - 1; j++)
//        values[idx * (vd - 1) + j] = value[j] * weight;
}

// values and position must already be device pointers
//void Permutohedral_Lattice::filter(float* output, const float* inputs, const float*  positions, bool reverse){
//    dim3 blocks((n - 1) / BLOCK_SIZE + 1, 1, 1);
//    dim3 blockSize(BLOCK_SIZE, 1, 1);
//    int cleanBlockSize = 128;
//    dim3 cleanBlocks((n - 1) / cleanBlockSize + 1, 2 * (pd + 1), 1);
//
//    createLattice<<<blocks, blockSize, 0, stream>>>(n, positions, scaleFactor, matrix, hashTable);
//    auto code = cudaGetLastError();
//    if(cudaSuccess != code) {
//        fprintf(stderr, "GPU Error: %s\n", cudaGetErrorString(code));
//        exit(code);
//    }
////    std::cout << "lattice gen" << std::endl;
//    cleanHashTable <<<cleanBlocks, cleanBlockSize, 0, stream>>>(2 * n * (pd + 1), hashTable);
//    code = cudaGetLastError();
//    if(cudaSuccess != code) {
//        fprintf(stderr, "GPU Error: %s\n", cudaGetErrorString(code));
//        exit(code);
//    }
////    std::cout << "clean hash" << std::endl;
////        cudaErrorCheck();
////
//    blocks.y = pd + 1;
//    splatCache<<<blocks, blockSize, 0, stream>>>(n, inputs, matrix, hashTable, &M);
//    code = cudaGetLastError();
//    if(cudaSuccess != code) {
//        fprintf(stderr, "GPU Error: %s\n", cudaGetErrorString(code));
//        exit(code);
//    }
////    std::cout << "splat cache" << std::endl;
////        cudaErrorCheck();
//
//    // é€ä¸ªç»´åº¦è¿›è¡Œblur
//    for (int remainder=reverse?pd:0; remainder >= 0 && remainder <= pd; reverse?remainder--:remainder++) {
//        blur<<<cleanBlocks, cleanBlockSize, 0, stream>>>(n * (pd + 1), newValues, matrix, remainder, hashTable);
//        code = cudaGetLastError();
//        if(cudaSuccess != code) {
//            fprintf(stderr, "GPU Error: %s\n", cudaGetErrorString(code));
//            exit(code);
//        }
//        std::swap(hashTable.values, newValues);
//    }
////    std::cout << "blur over" << std::endl;
//    blockSize.y = 1;
//    slice<<<blocks, blockSize, 0, stream>>>(n, output, matrix, hashTable);
//    code = cudaGetLastError();
//    if(cudaSuccess != code) {
//        fprintf(stderr, "GPU Error: %s\n", cudaGetErrorString(code));
//        exit(code);
//    }
////    std::cout << "slice over" << std::endl;
//}

void Permutohedral_Lattice::Initialization(const float*  positions){
    // åˆ†é…è®¾å¤‡å†…å­˜
    cudaMalloc((void **)&d_M, sizeof(int));
    // ä»ä¸»æœºå¤åˆ¶æ•°æ®åˆ°è®¾å¤‡
    cudaMemcpy(d_M, &M, sizeof(int), cudaMemcpyHostToDevice);

    dim3 blocks((n - 1) / BLOCK_SIZE + 1, 1, 1);
    dim3 blockSize(BLOCK_SIZE, 1, 1);
    int cleanBlockSize = 128;
    dim3 cleanBlocks((n - 1) / cleanBlockSize + 1, 2 * (pd + 1), 1);

    createLattice<<<blocks, blockSize, 0, stream>>>(n, positions, scaleFactor, matrix, hashTable);
    auto code = cudaGetLastError();
    if(cudaSuccess != code) {
        fprintf(stderr, "GPU Error: %s\n", cudaGetErrorString(code));
        exit(code);
    }
//    std::cout << "lattice gen" << std::endl;
    cleanHashTable <<<cleanBlocks, cleanBlockSize, 0, stream>>>(2 * n * (pd + 1), hashTable, d_M);
    code = cudaGetLastError();
    if(cudaSuccess != code) {
        fprintf(stderr, "GPU Error: %s\n", cudaGetErrorString(code));
        exit(code);
    }
    // ä»è®¾å¤‡å¤åˆ¶æ•°æ®å›ä¸»æœº
    cudaMemcpy(&M, d_M, sizeof(int), cudaMemcpyDeviceToHost);
    cudaFree(d_M);

    blocks.y = pd + 1;
    update_matrix<<<blocks, blockSize, 0, stream>>>(n, matrix, hashTable);
//    std::cout << "clean hash" << std::endl;
//        cudaErrorCheck();
//

}

void Permutohedral_Lattice::Splat(const float *inputs) {
    dim3 blocks((n - 1) / BLOCK_SIZE + 1, 1, 1);
    dim3 blockSize(BLOCK_SIZE, 1, 1);

    blocks.y = pd + 1;
    splatCache<<<blocks, blockSize, 0, stream>>>(n, inputs, matrix, hashTable);
    // åŒæ­¥è®¾å¤‡
    cudaDeviceSynchronize();

    auto code = cudaGetLastError();
    if(cudaSuccess != code) {
        fprintf(stderr, "GPU Error: %s\n", cudaGetErrorString(code));
        exit(code);
    }


}

void Permutohedral_Lattice::Blur(bool reverse) {
//    dim3 blocks((n - 1) / BLOCK_SIZE + 1, 1, 1);
//    dim3 blockSize(BLOCK_SIZE, 1, 1);
    int cleanBlockSize = 128;
    dim3 cleanBlocks((n - 1) / cleanBlockSize + 1, 2 * (pd + 1), 1);
    // é€ä¸ªç»´åº¦è¿›è¡Œblur
    for (int remainder=reverse?pd:0; remainder >= 0 && remainder <= pd; reverse?remainder--:remainder++) {
        blur<<<cleanBlocks, cleanBlockSize, 0, stream>>>(n * (pd + 1), newValues, matrix, remainder, hashTable);
        auto code = cudaGetLastError();
        if(cudaSuccess != code) {
            fprintf(stderr, "GPU Error: %s\n", cudaGetErrorString(code));
            exit(code);
        }
        std::swap(hashTable.values, newValues);
    }
}

void Permutohedral_Lattice::Slice(float *output, bool reverse) {
    dim3 blocks((n - 1) / BLOCK_SIZE + 1, 1, 1);
    dim3 blockSize(BLOCK_SIZE, 1, 1);
//    int cleanBlockSize = 128;
//    dim3 cleanBlocks((n - 1) / cleanBlockSize + 1, 2 * (pd + 1), 1);

    blockSize.y = 1;
    slice<<<blocks, blockSize, 0, stream>>>(n, output, matrix, hashTable);
    auto code = cudaGetLastError();
    if(cudaSuccess != code) {
        fprintf(stderr, "GPU Error: %s\n", cudaGetErrorString(code));
        exit(code);
    }

    // ä½¿ç”¨cudaMemsetå°†æ•°ç»„è®¾ç½®ä¸º0
    cudaError_t status = cudaMemset(hashTable.values, 0, hashTable.capacity * vd * sizeof(float));

    // æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯å‘ç”Ÿ
    if (status != cudaSuccess) {
        // å¤„ç†é”™è¯¯
    }
}


